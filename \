use lazy_static::lazy_static;
use rand::prelude::*;
use rayon::iter::IntoParallelIterator;
use rayon::iter::ParallelIterator;

pub const BETA: f32 = 1.0;
pub const NUM_VERTICES: usize = 100 * 100;
pub const PAYOFF_MATRIX: [[i32; 3]; 3] = [[1, 0, 2], [2, 1, 0], [0, 2, 1]];

lazy_static! {
    static ref EXP_TABLE: [f32; 2001] = {

        // exp above 89 gives inf
        let mut table = [0.0; 2001];
        for i in -1000..=1000 {
            table[(i + 1000) as usize] = (i as f32 * BETA).exp();
        }
        table
    };
}

pub fn exp(n: i32) -> f32 {
    if -1000 <= n && n <= 1000 {
        return EXP_TABLE[(n + 1000) as usize];
    };
    (n as f32 * BETA).exp()
}
//  an adjacency matrix with a constant,runtime set number of vertices
pub struct Network {
    edges: Vec<(usize, usize)>,
}

impl Default for Network {
    fn default() -> Self {
        Self::new()
    }
}

impl Network {
    pub fn new() -> Self {
        let mut edges = vec![];
        for i in 0..NUM_VERTICES {
            edges.extend_from_slice(&[(i, (i + 1) % NUM_VERTICES)]);
        }
        Network { edges }
    }
}

#[derive(Clone, Copy, Debug, PartialEq)]
pub enum Strategy {
    Rock = 0,
    Paper = 1,
    Scissors = 2,
}

pub fn play_game(agent_1_strategy: Strategy, agent_2_strategy: Strategy) -> (i32, i32) {
    // create a payoff matrix
    let agent1_strategy = agent_1_strategy as usize;
    let agent2_strategy = agent_2_strategy as usize;
    let agent1_payoff = PAYOFF_MATRIX[agent1_strategy][agent2_strategy];
    let agent2_payoff = PAYOFF_MATRIX[agent2_strategy][agent1_strategy];
    (agent1_payoff, agent2_payoff)
}

pub fn play_tournament(strategies: &[Strategy], scores: &mut [i32], network: &Network) {
    for (agent_1_id, agent_2_id) in network.edges.iter() {
        let (agent_1_payoff, agent2_payoff) =
            play_game(strategies[*agent_1_id], strategies[*agent_2_id]);
        scores[*agent_1_id] += agent_1_payoff;
        scores[*agent_2_id] += agent2_payoff;
    }
}

pub fn get_local_scores(
    strategies: &[Strategy],
    scores: &[i32],
    network: &Network,
) -> Vec<[i32; 3]> {
    // returns an array whos elements are the local coinstructed sciore of each strategy  as
    // measured by the onode at that index.
    let mut results = vec![[0, 0, 0]; NUM_VERTICES];
    for (agent1_id, agent2_id) in network.edges.iter() {
        let (agent_1_strategy, agent_2_strategy) = (strategies[*agent1_id], strategies[*agent2_id]);

        results[*agent2_id][agent_1_strategy as usize] += scores[*agent1_id];
        results[*agent1_id][agent_2_strategy as usize] += scores[*agent2_id];
    }

    for agent_id in 0..NUM_VERTICES {
        results[agent_id][strategies[agent_id] as usize] += scores[agent_id];
    }

    results
}

pub fn get_new_strat(random_number: f32, scores: &[i32; 3]) -> Strategy {
    let probabilities = [exp(scores[0]), exp(scores[1]), exp(scores[2])];

    // Calculate total without cloning
    let total: f32 = probabilities.iter().sum();
    let threshold = random_number * total;

    // Use early returns to avoid unnecessary iterations
    let mut cumulative = probabilities[0];
    if threshold < cumulative {
        return Strategy::Rock;
    }

    cumulative += probabilities[1];
    if threshold < cumulative {
        return Strategy::Paper;
    }

    // No need to check the last condition
    Strategy::Scissors
}

pub fn update_strategies(strategies: &mut Vec<Strategy>, scores: &[i32], network: &Network) {
    let mut rng = rand::rng();
    let random_numbers = (0..NUM_VERTICES)
        .map(|_| rng.random::<f32>())
        .collect::<Vec<_>>();

    let local_scores = get_local_scores(strategies, scores, network);
    let new_strats = (0..NUM_VERTICES)
        .into_par_iter()
        .map(|i| get_new_strat(random_numbers[i], &local_scores[i]))
        .collect::<Vec<_>>();
    *strategies = new_strats;
}

#[cfg(test)]
mod tests {
    use super::*;

    // Helper to create a custom test Network.
    fn test_network(edges: Vec<(usize, usize)>) -> Network {
        // Bypass the default constructor by directly providing the edges.
        Network { edges }
    }

    #[test]
    fn test_play_game_mechanics() {
        // Test a known matchup: Rock vs. Paper.
        let (score1, score2) = play_game(Strategy::Rock, Strategy::Paper);
        let expected1 = PAYOFF_MATRIX[Strategy::Rock as usize][Strategy::Paper as usize];
        let expected2 = PAYOFF_MATRIX[Strategy::Paper as usize][Strategy::Rock as usize];
        assert_eq!(score1, expected1);
        assert_eq!(score2, expected2);
    }

    #[test]
    fn test_get_local_scores_mechanics() {
        // Construct a small network with 3 vertices.
        // Edges: (0,1), (1,2), (2,0)
        let network = test_network(vec![(0, 1), (1, 2), (2, 0)]);
        let strategies = vec![Strategy::Rock, Strategy::Paper, Strategy::Scissors];
        let scores = vec![10, 20, 30];

        // Expected local scores:
        // For edge (0,1):
        //   Vertex 1: add score[0]=10 to index Rock.
        //   Vertex 0: add score[1]=20 to index Paper.
        // For edge (1,2):
        //   Vertex 2: add score[1]=20 to index Paper.
        //   Vertex 1: add score[2]=30 to index Scissors.
        // For edge (2,0):
        //   Vertex 0: add score[2]=30 to index Scissors.
        //   Vertex 2: add score[0]=10 to index Rock.
        let expected = vec![[10, 20, 30], [10, 20, 30], [10, 20, 30]];

        let computed = get_local_scores(&strategies, &scores, &network);
        assert_eq!(computed, expected);
    }

    #[test]
    fn test_get_new_strat_mechanics() {
        // With equal scores (0,0,0) probabilities are [1,1,1].
        // Total = 3.
        // For random number 0.2, threshold=0.6 (<1) → Rock.
        assert_eq!(get_new_strat(0.2, &[0, 0, 0]), Strategy::Rock);
        // For random number 0.4, threshold=1.2 (falls in second slot) → Paper.
        assert_eq!(get_new_strat(0.4, &[0, 0, 0]), Strategy::Paper);
        // For random number 0.9, threshold=2.7 (third slot) → Scissors.
        assert_eq!(get_new_strat(0.9, &[0, 0, 0]), Strategy::Scissors);
    }

    #[test]
    fn test_play_tournament_mechanics() {
        // Create a 2-vertex network with two directed edges.
        let network = test_network(vec![(0, 1), (1, 0)]);
        let strategies = vec![Strategy::Rock, Strategy::Paper];
        let mut scores = vec![0, 0];
        play_tournament(&strategies, &mut scores, &network);
        // For edge (0,1): Rock vs. Paper yields:
        //    agent0: PAYOFF_MATRIX[Rock][Paper], agent1: PAYOFF_MATRIX[Paper][Rock]
        // For edge (1,0): the same again.
        let expected_score0 = PAYOFF_MATRIX[Strategy::Rock as usize][Strategy::Paper as usize] * 2;
        let expected_score1 = PAYOFF_MATRIX[Strategy::Paper as usize][Strategy::Rock as usize] * 2;
        assert_eq!(scores, vec![expected_score0, expected_score1]);
    }

    #[test]
    fn test_update_strategies_statistical() {
        let iterations = 1000;
        let mut counts = [0; 3]; // counts for Rock, Paper, Scissors
        // Create a network with a single vertex and self-edge.
        let network = test_network(vec![(0, 0)]);
        let scores = vec![0]; // Equal scores yield uniform probabilities.
        for _ in 0..iterations {
            // Start with an arbitrary strategy.
            let mut strategies = vec![Strategy::Rock];
            update_strategies(&mut strategies, &scores, &network);
            match strategies[0] {
                Strategy::Rock => counts[0] += 1,
                Strategy::Paper => counts[1] += 1,
                Strategy::Scissors => counts[2] += 1,
            }
        }
        // With uniform probabilities, each strategy should be chosen ~iterations/3 times.
        let expected = iterations as f32 / 3.0;
        let tolerance = iterations as f32 * 0.1; // 10% tolerance.
        for (i, &count) in counts.iter().enumerate() {
            assert!(
                (count as f32 - expected).abs() <= tolerance,
                "Strategy {} occurred {} times; expected approx {} (±{})",
                i,
                count,
                expected,
                tolerance
            );
        }
    }
}
